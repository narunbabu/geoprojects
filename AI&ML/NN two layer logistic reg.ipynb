{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Logical Operation by 2-layer Neural Networks (Logistic Regression) on TensorFlow\n",
    "Author: Rowel Atienza\n",
    "Project: https://github.com/roatienza/Deep-Learning-Experiments\n",
    "'''\n",
    "# On command line: python3 logic_gate_logits.py\n",
    "# Prerequisite: tensorflow 1.0 (see tensorflow.org)\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "learning_rate = 0.3\n",
    "x_data = np.reshape(np.array( [[0., 0.], [0., 1.], [1., 0.], [1., 1.]], dtype=np.float32 ),[4,2])\n",
    "# try other logics; xor = [0., 1., 1., 0.], or = [0., 1., 1., 1.], and = [0., 0., 0., 1.], etc\n",
    "logic_out = np.array([0., 1., 1., 0.], dtype=np.float32)\n",
    "y_data = np.reshape(logic_out,[4,1])\n",
    "np.array( [[0., 0.], [0., 1.], [1., 0.], [1., 1.]], dtype=np.float32 ),x_data,y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from get_log_data import read_logs, get_labelized_logdata\n",
    "import pandas as pd\n",
    "X_train, X_test, y_train, y_test=get_labelized_logdata(4)\n",
    "\n",
    "x_data = np.reshape(np.array( [[0., 0.], [0., 1.], [1., 0.], [1., 1.]], dtype=np.float32 ),[4,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = y_data.shape[0]\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=(None, 2))\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "# try other values for nhidden\n",
    "nhidden = 16\n",
    "W0 = tf.Variable(tf.random_normal([2, nhidden],stddev=0.1))\n",
    "b0 = tf.Variable(tf.zeros([nhidden]))\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([nhidden, 1],stddev=0.1))\n",
    "b1 = tf.Variable(tf.zeros([1]))\n",
    "\n",
    "hidden = tf.matmul(x, W0) + b0\n",
    "yp = tf.matmul(tf.nn.relu(hidden), W1) + b1\n",
    "logits = tf.nn.softmax(yp,dim=0)\n",
    "\n",
    "entropy = -tf.multiply(y,tf.log(logits))\n",
    "loss = tf.reduce_mean(entropy)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "train_step = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    for i in range(1000):\n",
    "        # mini-batch can also be used but we have a small set of data only\n",
    "        # offset = (i*2)%(n-2)\n",
    "        # feed_dict ={x:x_data[offset:(offset+2),:], y:y_data[offset:(offset+2)]}\n",
    "        # so we use all data during training\n",
    "        feed_dict = {x: x_data[:,:], y: y_data[:]}\n",
    "        _, l, y_, yp_ = session.run([train_step, loss, y, logits],feed_dict=feed_dict)\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(\"--- %d: Loss = %lf\" % (i+1, l))\n",
    "    # Let's validate if we get the correct output given an input\n",
    "    print(\"In: \")\n",
    "    # You can choose all inputs (0:4) or some by modifying the range eg (1:2)\n",
    "    input = x_data[0:4,:]\n",
    "    print(input)\n",
    "    hidden = tf.matmul(input, W0) + b0\n",
    "    print(\"Predicted output:\")\n",
    "    yp = tf.nn.softmax(tf.matmul(tf.nn.relu(hidden), W1) + b1,dim=0)\n",
    "    print(print(1*np.greater(yp.eval(),0.25)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfgpu15",
   "language": "python",
   "name": "tfgpu15"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
