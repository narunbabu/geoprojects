{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Deep Neural Networks on Not MNIST using Keras\n",
    "Author: Rowel Atienza\n",
    "Project: https://github.com/roatienza/Deep-Learning-Experiments\n",
    "\"\"\"\n",
    "# On command line: python3 mnist_a2j_mlp_keras.py\n",
    "# Prerequisite: tensorflow 1.0 and keras 2.0\n",
    "# must run mnist_a2j_2pickle.py first (one-time) to generate the data\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "start_time = time.time()\n",
    "def elapsed(sec):\n",
    "    if sec<60:\n",
    "        return str(sec) + \" sec\"\n",
    "    elif sec<(60*60):\n",
    "        return str(sec/60) + \" min\"\n",
    "    else:\n",
    "        return str(sec/(60*60)) + \" hr\"\n",
    "\n",
    "## use of pickle to speed up loading of data\n",
    "# pickle_file = open( \"mnist_a2j.pickle\", \"rb\" )\n",
    "# data = pickle.load(pickle_file)\n",
    "# test_labels = data[\"test_labels\"]\n",
    "# train_labels = data[\"all_labels\"]\n",
    "# test_dataset = data[\"test_dataset\"]\n",
    "# train_dataset = data[\"all_dataset\"]\n",
    "# del data\n",
    "# pickle_file.close()\n",
    "\n",
    "# print(\"Training size: \", train_dataset.shape)\n",
    "# print(\"Training labels: \", train_labels.shape)\n",
    "# print(\"Test size: \", test_dataset.shape)\n",
    "# print(\"Test labels: \", test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_log_data import read_logs, get_log_traintest,get_digitized_logdata, get_labelized_logdata\n",
    "x_train, x_test, y_train, y_test, scaler_x, scaler_y=get_log_traintest('phit',True,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.]), array([0.405632]), array([0.]), array([0.41]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(y_train),max(y_train),min(y_test),max(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_log_data import read_logs, get_log_traintest,get_digitized_logdata, get_labelized_logdata\n",
    "num_labels =2# train_labels.shape[1]\n",
    "barrier=0.3\n",
    "train_dataset, test_dataset, train_labels, test_labels=get_labelized_logdata(num_labels,por_barrier)\n",
    "get_labelized_logdata(n_labels,ref=barrier,LABEL='phit')\n",
    "\n",
    "image_size = 28\n",
    "input_size = 7 #image_size*image_size\n",
    "batch_size = 1028\n",
    "hidden_units = 1024\n",
    "learning_rate = 0.5\n",
    "dropout = 0.8\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(hidden_units, input_dim=input_size))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Dense(hidden_units))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "sgd = SGD(lr=learning_rate) # , decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_dataset, train_labels,\n",
    "          epochs=75,\n",
    "          batch_size=batch_size, shuffle=False)\n",
    "score = np.asarray(model.evaluate(test_dataset, test_labels, batch_size=batch_size))*100.0\n",
    "# Accuracy: 86.0%\n",
    "print(\"\\nTest accuracy: %.1f%%\" % score[1])\n",
    "print(\"Elapsed: \" , elapsed(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(test_dataset),test_labels\n",
    "test_pred=np.array([0 if b<0.5 else 1 for a,b in model.predict(test_dataset)])\n",
    "print(sum(test_labels-test_pred))\n",
    "count=0\n",
    "neg_count=0\n",
    "for a, b in zip(test_labels,test_pred):\n",
    "    if a==b:\n",
    "        count +=1\n",
    "        \n",
    "    else:\n",
    "        neg_count +=1\n",
    "count,neg_count,count+neg_count,count/(count+neg_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print(confusion_matrix(test_labels,test_pred))\n",
    "print(classification_report(test_labels,test_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfgpu15",
   "language": "python",
   "name": "tfgpu15"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
