{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Deep Neural Networks on Not MNIST using Keras\n",
    "Author: Rowel Atienza\n",
    "Project: https://github.com/roatienza/Deep-Learning-Experiments\n",
    "\"\"\"\n",
    "# On command line: python3 mnist_a2j_mlp_keras.py\n",
    "# Prerequisite: tensorflow 1.0 and keras 2.0\n",
    "# must run mnist_a2j_2pickle.py first (one-time) to generate the data\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "start_time = time.time()\n",
    "def elapsed(sec):\n",
    "    if sec<60:\n",
    "        return str(sec) + \" sec\"\n",
    "    elif sec<(60*60):\n",
    "        return str(sec/60) + \" min\"\n",
    "    else:\n",
    "        return str(sec/(60*60)) + \" hr\"\n",
    "\n",
    "## use of pickle to speed up loading of data\n",
    "# pickle_file = open( \"mnist_a2j.pickle\", \"rb\" )\n",
    "# data = pickle.load(pickle_file)\n",
    "# test_labels = data[\"test_labels\"]\n",
    "# train_labels = data[\"all_labels\"]\n",
    "# test_dataset = data[\"test_dataset\"]\n",
    "# train_dataset = data[\"all_dataset\"]\n",
    "# del data\n",
    "# pickle_file.close()\n",
    "\n",
    "# print(\"Training size: \", train_dataset.shape)\n",
    "# print(\"Training labels: \", train_labels.shape)\n",
    "# print(\"Test size: \", test_dataset.shape)\n",
    "# print(\"Test labels: \", test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_log_data import read_logs, get_log_traintest,get_digitized_logdata, get_labelized_logdata\n",
    "x_train, x_test, y_train, y_test, scaler_x, scaler_y=get_log_traintest('vwcl',True,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.]), array([1.]), array([0.]), array([1.]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(y_train),max(y_train),min(y_test),max(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7401],\n",
       "       [0.7354],\n",
       "       [0.7529],\n",
       "       ...,\n",
       "       [0.6602],\n",
       "       [0.6635],\n",
       "       [0.6871]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "14853/14853 [==============================] - 1s 69us/step - loss: 0.3636 - acc: 0.8453\n",
      "Epoch 2/75\n",
      "14853/14853 [==============================] - 1s 38us/step - loss: 0.1533 - acc: 0.9346\n",
      "Epoch 3/75\n",
      "14853/14853 [==============================] - 1s 41us/step - loss: 0.1531 - acc: 0.9329\n",
      "Epoch 4/75\n",
      "14853/14853 [==============================] - 1s 36us/step - loss: 0.1446 - acc: 0.9366\n",
      "Epoch 5/75\n",
      "14853/14853 [==============================] - 1s 36us/step - loss: 0.1320 - acc: 0.9416\n",
      "Epoch 6/75\n",
      "14853/14853 [==============================] - 1s 37us/step - loss: 0.1260 - acc: 0.9450: 0s - loss: 0.1827 - acc: \n",
      "Epoch 7/75\n",
      "14853/14853 [==============================] - 1s 38us/step - loss: 0.1232 - acc: 0.9463\n",
      "Epoch 8/75\n",
      "14853/14853 [==============================] - 1s 40us/step - loss: 0.1168 - acc: 0.9502\n",
      "Epoch 9/75\n",
      "14853/14853 [==============================] - 1s 39us/step - loss: 0.1149 - acc: 0.9506\n",
      "Epoch 10/75\n",
      "14853/14853 [==============================] - 1s 38us/step - loss: 0.1112 - acc: 0.9508\n",
      "Epoch 11/75\n",
      "14853/14853 [==============================] - 1s 37us/step - loss: 0.1108 - acc: 0.9501\n",
      "Epoch 12/75\n",
      "14853/14853 [==============================] - 1s 38us/step - loss: 0.1088 - acc: 0.9516\n",
      "Epoch 13/75\n",
      "14853/14853 [==============================] - 1s 39us/step - loss: 0.1061 - acc: 0.9534\n",
      "Epoch 14/75\n",
      "14853/14853 [==============================] - 1s 40us/step - loss: 0.1049 - acc: 0.9536: 0s - loss: 0.1555 - acc: \n",
      "Epoch 15/75\n",
      "14853/14853 [==============================] - 1s 42us/step - loss: 0.1077 - acc: 0.9537\n",
      "Epoch 16/75\n",
      "14853/14853 [==============================] - 1s 39us/step - loss: 0.1029 - acc: 0.9543\n",
      "Epoch 17/75\n",
      "14853/14853 [==============================] - 1s 38us/step - loss: 0.1010 - acc: 0.9546\n",
      "Epoch 18/75\n",
      "14853/14853 [==============================] - 1s 39us/step - loss: 0.1056 - acc: 0.9546\n",
      "Epoch 19/75\n",
      "14853/14853 [==============================] - 1s 38us/step - loss: 0.1040 - acc: 0.9562\n",
      "Epoch 20/75\n",
      "14853/14853 [==============================] - 1s 38us/step - loss: 0.0985 - acc: 0.9574\n",
      "Epoch 21/75\n",
      "14853/14853 [==============================] - 1s 43us/step - loss: 0.0993 - acc: 0.9566: 0s - loss: 0.1213 - acc: 0.9\n",
      "Epoch 22/75\n",
      "14853/14853 [==============================] - 1s 45us/step - loss: 0.0982 - acc: 0.9583\n",
      "Epoch 23/75\n",
      "14853/14853 [==============================] - 1s 36us/step - loss: 0.1007 - acc: 0.9572\n",
      "Epoch 24/75\n",
      "14853/14853 [==============================] - 1s 37us/step - loss: 0.0987 - acc: 0.9560\n",
      "Epoch 25/75\n",
      "14853/14853 [==============================] - 1s 36us/step - loss: 0.0965 - acc: 0.9587\n",
      "Epoch 26/75\n",
      "14853/14853 [==============================] - 1s 37us/step - loss: 0.0963 - acc: 0.9578\n",
      "Epoch 27/75\n",
      "14853/14853 [==============================] - 1s 37us/step - loss: 0.0968 - acc: 0.9583\n",
      "Epoch 28/75\n",
      "14853/14853 [==============================] - 1s 36us/step - loss: 0.0977 - acc: 0.9568\n",
      "Epoch 29/75\n",
      "14853/14853 [==============================] - 1s 36us/step - loss: 0.0956 - acc: 0.9587\n",
      "Epoch 30/75\n",
      "14853/14853 [==============================] - 1s 37us/step - loss: 0.0933 - acc: 0.9595\n",
      "Epoch 31/75\n",
      "14853/14853 [==============================] - 1s 36us/step - loss: 0.0952 - acc: 0.9590\n",
      "Epoch 32/75\n",
      "14853/14853 [==============================] - 1s 37us/step - loss: 0.0952 - acc: 0.9582\n",
      "Epoch 33/75\n",
      "14853/14853 [==============================] - 1s 36us/step - loss: 0.0939 - acc: 0.9603\n",
      "Epoch 34/75\n",
      "14853/14853 [==============================] - 1s 35us/step - loss: 0.0952 - acc: 0.9595\n",
      "Epoch 35/75\n",
      "14853/14853 [==============================] - 1s 34us/step - loss: 0.0946 - acc: 0.9580\n",
      "Epoch 36/75\n",
      "14853/14853 [==============================] - 1s 35us/step - loss: 0.0913 - acc: 0.9597\n",
      "Epoch 37/75\n",
      "14853/14853 [==============================] - 1s 34us/step - loss: 0.0922 - acc: 0.9597\n",
      "Epoch 38/75\n",
      "14853/14853 [==============================] - 1s 36us/step - loss: 0.0954 - acc: 0.9591\n",
      "Epoch 39/75\n",
      "14853/14853 [==============================] - 1s 35us/step - loss: 0.0930 - acc: 0.9609\n",
      "Epoch 40/75\n",
      "14853/14853 [==============================] - 1s 34us/step - loss: 0.0907 - acc: 0.9612\n",
      "Epoch 41/75\n",
      "14853/14853 [==============================] - 1s 48us/step - loss: 0.0897 - acc: 0.9618\n",
      "Epoch 42/75\n",
      "14853/14853 [==============================] - 1s 37us/step - loss: 0.0903 - acc: 0.9602\n",
      "Epoch 43/75\n",
      "14853/14853 [==============================] - 0s 33us/step - loss: 0.0921 - acc: 0.9601\n",
      "Epoch 44/75\n",
      "14853/14853 [==============================] - 1s 36us/step - loss: 0.0913 - acc: 0.9618\n",
      "Epoch 45/75\n",
      "14853/14853 [==============================] - 0s 32us/step - loss: 0.0917 - acc: 0.9596\n",
      "Epoch 46/75\n",
      "14853/14853 [==============================] - 0s 34us/step - loss: 0.0886 - acc: 0.9621\n",
      "Epoch 47/75\n",
      "14853/14853 [==============================] - 1s 34us/step - loss: 0.0887 - acc: 0.9624\n",
      "Epoch 48/75\n",
      "14853/14853 [==============================] - 1s 34us/step - loss: 0.0902 - acc: 0.9614\n",
      "Epoch 49/75\n",
      "14853/14853 [==============================] - 1s 34us/step - loss: 0.0885 - acc: 0.9629\n",
      "Epoch 50/75\n",
      "14853/14853 [==============================] - 1s 35us/step - loss: 0.0890 - acc: 0.9624\n",
      "Epoch 51/75\n",
      "14853/14853 [==============================] - 1s 37us/step - loss: 0.0898 - acc: 0.9616\n",
      "Epoch 52/75\n",
      "14853/14853 [==============================] - 1s 37us/step - loss: 0.0886 - acc: 0.9636\n",
      "Epoch 53/75\n",
      "14853/14853 [==============================] - 1s 36us/step - loss: 0.0894 - acc: 0.9623\n",
      "Epoch 54/75\n",
      "14853/14853 [==============================] - 1s 34us/step - loss: 0.0900 - acc: 0.9627\n",
      "Epoch 55/75\n",
      "14853/14853 [==============================] - 0s 33us/step - loss: 0.0890 - acc: 0.9628\n",
      "Epoch 56/75\n",
      "14853/14853 [==============================] - 1s 35us/step - loss: 0.0873 - acc: 0.9624\n",
      "Epoch 57/75\n",
      "14853/14853 [==============================] - 1s 36us/step - loss: 0.0866 - acc: 0.9624\n",
      "Epoch 58/75\n",
      "14853/14853 [==============================] - 1s 35us/step - loss: 0.0884 - acc: 0.9615\n",
      "Epoch 59/75\n",
      "14853/14853 [==============================] - 1s 36us/step - loss: 0.0868 - acc: 0.9622\n",
      "Epoch 60/75\n",
      "14853/14853 [==============================] - 1s 36us/step - loss: 0.0855 - acc: 0.9625\n",
      "Epoch 61/75\n",
      "14853/14853 [==============================] - 1s 50us/step - loss: 0.0881 - acc: 0.9631\n",
      "Epoch 62/75\n",
      "14853/14853 [==============================] - 1s 34us/step - loss: 0.0860 - acc: 0.9638\n",
      "Epoch 63/75\n",
      "14853/14853 [==============================] - 1s 35us/step - loss: 0.0855 - acc: 0.9632\n",
      "Epoch 64/75\n",
      "14853/14853 [==============================] - 0s 33us/step - loss: 0.0878 - acc: 0.9621\n",
      "Epoch 65/75\n",
      "14853/14853 [==============================] - 1s 35us/step - loss: 0.0855 - acc: 0.9636\n",
      "Epoch 66/75\n",
      "14853/14853 [==============================] - 1s 35us/step - loss: 0.0853 - acc: 0.9633\n",
      "Epoch 67/75\n",
      "14853/14853 [==============================] - 1s 35us/step - loss: 0.0850 - acc: 0.9635\n",
      "Epoch 68/75\n",
      "14853/14853 [==============================] - 1s 36us/step - loss: 0.0865 - acc: 0.9636\n",
      "Epoch 69/75\n",
      "14853/14853 [==============================] - 1s 37us/step - loss: 0.0853 - acc: 0.9639\n",
      "Epoch 70/75\n",
      "14853/14853 [==============================] - 1s 37us/step - loss: 0.0854 - acc: 0.9637\n",
      "Epoch 71/75\n",
      "14853/14853 [==============================] - 1s 38us/step - loss: 0.0850 - acc: 0.9650\n",
      "Epoch 72/75\n",
      "14853/14853 [==============================] - 1s 37us/step - loss: 0.0854 - acc: 0.9640\n",
      "Epoch 73/75\n",
      "14853/14853 [==============================] - 1s 34us/step - loss: 0.0861 - acc: 0.9621\n",
      "Epoch 74/75\n",
      "14853/14853 [==============================] - 1s 37us/step - loss: 0.0856 - acc: 0.9649\n",
      "Epoch 75/75\n",
      "14853/14853 [==============================] - 1s 35us/step - loss: 0.0858 - acc: 0.9632\n",
      "1887/1887 [==============================] - 0s 43us/step\n",
      "\n",
      "Test accuracy: 94.5%\n",
      "Elapsed:  2.5884839057922364 min\n"
     ]
    }
   ],
   "source": [
    "from get_log_data import read_logs, get_log_traintest,get_digitized_logdata, get_labelized_logdata\n",
    "num_labels =2# train_labels.shape[1]\n",
    "barrier=0.5\n",
    "train_dataset, test_dataset, train_labels, test_labels=get_labelized_logdata(num_labels,ref=barrier,LABEL='vwcl') #LABEL='phit'\n",
    "\n",
    "image_size = 28\n",
    "input_size = 7 #image_size*image_size\n",
    "batch_size = 1028\n",
    "hidden_units = 1024\n",
    "learning_rate = 0.5\n",
    "dropout = 0.8\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(hidden_units, input_dim=input_size))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Dense(hidden_units))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "sgd = SGD(lr=learning_rate) # , decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_dataset, train_labels,\n",
    "          epochs=75,\n",
    "          batch_size=batch_size, shuffle=False)\n",
    "score = np.asarray(model.evaluate(test_dataset, test_labels, batch_size=batch_size))*100.0\n",
    "# Accuracy: 86.0%\n",
    "print(\"\\nTest accuracy: %.1f%%\" % score[1])\n",
    "print(\"Elapsed: \" , elapsed(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-95.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1784, 103, 1887, 0.9454160042395336)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(test_dataset),test_labels\n",
    "test_pred=np.array([0 if b<0.5 else 1 for a,b in model.predict(test_dataset)])\n",
    "print(sum(test_labels-test_pred))\n",
    "count=0\n",
    "neg_count=0\n",
    "for a, b in zip(test_labels,test_pred):\n",
    "    if a==b:\n",
    "        count +=1\n",
    "        \n",
    "    else:\n",
    "        neg_count +=1\n",
    "count,neg_count,count+neg_count,count/(count+neg_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 168   99]\n",
      " [   4 1616]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.98      0.63      0.77       267\n",
      "        1.0       0.94      1.00      0.97      1620\n",
      "\n",
      "avg / total       0.95      0.95      0.94      1887\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print(confusion_matrix(test_labels,test_pred))\n",
    "print(classification_report(test_labels,test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfgpu15",
   "language": "python",
   "name": "tfgpu15"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
